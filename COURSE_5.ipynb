{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK1      \n",
    "> ## 数学符号     \n",
    "\n",
    "$x^{<t>},y^{<t>}$代表输入输出的序列第$t$个时刻的位置    \n",
    "$x^{(i)<t>},y^{(i)<t>}$代表输入输出的序列第$i$个样本第$t$个时刻位置的值      \n",
    "$T_x,T_y$输入输出序列的长度    \n",
    "$T_x^{(i)},T_x^{(i)}$代表第$i$个训练样本输入/输出序列的长度\n",
    "\n",
    "> ## NLP      \n",
    "\n",
    "NLP的做法：      \n",
    "变量的每个维度代表一个句子的一个词：     \n",
    "1. 做一张词表/词典，包含你需要使用的词语（常用词）    \n",
    "2. 对输入序列One-hot编码（词典长度\\*序列长度）    \n",
    "3. 如果遇到不在词典中的词语——创作一个虚拟词汇（UNK,Unknown）     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_1.PNG\" width=600 height=600>     \n",
    "\n",
    "> ## 循环神经网络    \n",
    "\n",
    "### 传统神经网络面对序列识别的问题\n",
    "1. 输入序列非定长，填充不是一个好办法(pad会假设一个最大的文本长度)    \n",
    "2. 没有学习到不同位置的输入(比如文本不同位置的意思)的意义             \n",
    "    假设$x^{<1>}$处学到一个词的意思(比如断句),但当该词汇出现在$x^{<t>}$处，那么没办法识别到    \n",
    "    \n",
    "### 循环神经网络的基本工作    \n",
    "\n",
    "- 初始时刻输入一个激活值（通常是0）    \n",
    "从左到右读取一个句子（序列），那么这个第一个词的结构是$x^{<1>}$，当再读取$x^{<2>}$时会引入$x^{<1>}$的信息             \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_2.PNG\" width=600 height=600>  \n",
    "$W_{ax}$代表$x$到隐藏层的连接的一系列参数,循环连接/水平链接$W_{aa}$，输出链接$W_{ya}$    \n",
    "\n",
    "**从左到右扫描序列，所有的参数也是共享的**    \n",
    "\n",
    "**问题**：只考虑到之前的信息，没有考虑到之后的信息，$x^{<t>}$只考虑了$\\cdots,x^{<t-1>}$，没有考虑$x^{<t+1>}\\cdots$       \n",
    "\n",
    "### 循环神经网络前向传播       \n",
    "1. **初始化激活向量$a^{<0>}= \\vec{0}$**    \n",
    "\n",
    "\n",
    "2. **$a^{<t>} = g(w_{aa}a^{<t-1>}+w_{ax}x^{<t>}+b_a)=g(w_{a}[a^{<t-1>},x^{<t>}]^{T}+b_a)$**  \n",
    "    \n",
    "    其中$w_a = [w_{aa},w_{ax}]$\n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_3.PNG\" width=600 height=600> \n",
    "3. $\\hat y^{<t>} = g(w_{ya}a^{<t>}+b_y)$      \n",
    "\n",
    "    - 对于输出函数    \n",
    "        二分类——Sigmoid    \n",
    "        多分类——Softmax     \n",
    "        \n",
    "> ## 循环神经网络的反向传播          \n",
    "\n",
    "**单个时间步上的Lost function**：\n",
    "$$L^{<t>}(\\hat y^{<t>},y^{<t>}) = -y^{<t>}log\\hat y^{<t>}-(1-y^{<t>})log(1-\\hat y^{<t>})$$     \n",
    "**总的损失函数：所有单个时间步损失函数的求和**\n",
    "$$L(\\hat y,y) = \\sum ^{T_x}_{t=1}L^{<t>}(\\hat y^{<t>},y^{<t>})$$     \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_4.PNG\" width=600 height=600>      \n",
    "\n",
    "> ## 不同类型的循环神经网络    \n",
    "\n",
    "**RNN的类型：**   \n",
    "- 每个时间节点输入输出——多对多（或者全部输入结束后输出多个）    \n",
    "- 最后一个时间节点，每个时间节点输入——多对一    \n",
    "- 第一个时间节点输入，每个时间节点输出——一对多      \n",
    "\n",
    "对于机器翻译,输入或者输出的长度$T_x,T_y$可能不同     \n",
    "做法：读完句子以后在开始逐个输出（encoder，decoder）     \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_5.PNG\" width=600 height=600>      \n",
    "\n",
    "> ## 语言模型和序列生成     \n",
    "\n",
    "假设在做一个语音识别系统，识别出两句非常相似的话，要判断那句话是正确的，在这里需要一个语言模型来判断两句话的出现概率。\n",
    "\n",
    "### 语言模型     \n",
    "- 训练集：一个语料库         \n",
    "- 得到一个句子，首先把句子进行Tokenize（切分）后进行one-hot编码；    \n",
    "- 判断句子结束时通常添加一个额外的标记```<EOS>```      \n",
    "- 句子中的未知词替换成```<UNK>```未知词     \n",
    "    \n",
    "1. 第一个时间步:RNN读取零向量，输出$\\hat y^{<1>}$。      \n",
    "2. 下一个时间步:读取第一个词的正确结果(读取上一个时间步的结果,$x^{<t>}=y^{<t-1>}$)后在第一个词的条件概率下计算第二个词的概率；   \n",
    "3. 在前继词语的条件下预测第三个词的概率\n",
    "4. 最终建立一个**条件概率分布**判断序列的可能性     \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_6.PNG\" width=600 height=600> \n",
    "\n",
    "> ## 新序列采样     \n",
    "         \n",
    "训练出一个序列模型后，当需要确认该模型学到了什么，可以使用**新序列采样**的方法        \n",
    "\n",
    "1. 已有一个训练好的语言模型/序列模型\n",
    "2. 在第一个时间步输入$a^{<0>}=0$与$x^{<0>}=0$，得到一个softmax层的结果    \n",
    "3. 在softmax层的结果中根据每个单词的概率随机采样(```np.random.choice```)，输入下一个时间步    \n",
    "4. 下一个时间步输出的结果输入下下个时间步，重复直到出现```<EOS>```    \n",
    "5. 当遇到```<UNK>```时可以在此进行一次采样，直到不是```<UNK>```为止\n",
    "\n",
    "> ## 梯度消失    \n",
    "```\n",
    "例子:\n",
    "1. The cats ,which already ate......,was full\n",
    "2. The cats ,which already ate......,were full\n",
    "如何判断was和were\n",
    "```\n",
    "\n",
    "**基本RNN的缺点**：    \n",
    "RNN难以学习长期依赖的结果——前向传播以及反向传播时，前后端很难相互影响(长句子中，前端时间步的输入结果很难影响到后端，后端时间步词语的反向传播很难影响到前端时间步神经元的权重)      \n",
    "\n",
    "大型**深度**神经网络会在参数前后传播的过程中出现**指数级增长或减小**。                          \n",
    "\n",
    "- RNN出现梯度爆炸——Gradient clipping解决\n",
    "- 出现梯度消失——GRU，LSTM等\n",
    "\n",
    "\n",
    "\n",
    "> ## GRU(Gate Recurrent Unit)——门控循环单元       \n",
    "### 简单的GRU\n",
    "- $t$时间的激活函数输出：$a^{<t>}$       \n",
    "- 有一个memory cell，提供记忆的功能——在时间t时的记忆:$c^{<t>}=a^{<t>}$           \n",
    "- $c^{<t>}$的估计值：$\\tilde{c}^{<t>} = \\tanh(w_u[c^{<t-1>},x^{<t>}]+b_c)$         \n",
    "- 有一个Gate(0-1之间):$\\Gamma u = \\sigma(w_u[c^{<t-1>},x^{<t>}])$作用就是什么时候更新cell的值 ($\\tilde{c}^{<t>}$代替$c^{<t>}$)        \n",
    "- $c^{<t>} = \\Gamma u * \\tilde{c}^{<t>}+(1-\\Gamma u)*c^{<t-1>}$\n",
    "\n",
    "（$c^{<t>}$和$\\Gamma u$的维度一样）\n",
    "\n",
    "```假设一个句子：    \n",
    "The cat,which already ate...,was full.\n",
    "```\n",
    "Gate会在```cat```处更新cell为1，代表谓语应该是单数形式，维持cell的值直到```was full```，即在```was```之前所有地方，Gate应该维持关闭。     \n",
    "\n",
    "### 完整的GRU         \n",
    "- $t$时间的激活函数输出：$a^{<t>}$       \n",
    "- 有一个memory cell，提供记忆的功能——在时间t时的记忆:$c^{<t>}=a^{<t>}$           \n",
    "- $c^{<t>}$的估计值：$\\tilde{c}^{<t>} = \\tanh(w_u[\\Gamma_r c^{<t-1>},x^{<t>}]+b_c)$     \n",
    "- $\\Gamma_r = \\sigma(w_u[c^{<t-1>},x^{<t>}]+b_r)$(相关性)\n",
    "- 有一个Gate(0-1之间):$\\Gamma u = \\sigma(w_u[c^{<t-1>},x^{<t>}]+b_u)$作用就是什么时候更新cell的值 ($\\tilde{c}^{<t>}$代替$c^{<t>}$)        \n",
    "- $c^{<t>} = \\Gamma u * \\tilde{c}^{<t>}+(1-\\Gamma u)*c^{<t-1>}$\n",
    "\n",
    "（$c^{<t>}$和$\\Gamma u$的维度一样）\n",
    "\n",
    "\n",
    "\n",
    "> ## LSTM   \n",
    "\n",
    "相比于GRU，LSTM中$c^{<t>}\\neq a^{<t>}$ 且增加了两个门    \n",
    "- $t$时间的激活函数输出：$a^{<t>}$       \n",
    "- memory cell:$c^{<t>}=a^{<t>}$           \n",
    "- $c^{<t>}$的估计值：$\\tilde{c}^{<t>} = \\tanh(w_u[a^{<t-1>},x^{<t>}]+b_c)$     \n",
    "- update gate:$\\Gamma u = \\sigma(w_u[a^{<t-1>},x^{<t>}]+b_u)$\n",
    "- forget gate:$\\Gamma f = \\sigma(w_f[a^{<t-1>},x^{<t>}]+b_f)$\n",
    "- output gate:$\\Gamma o = \\sigma(w_o[a^{<t-1>},x^{<t>}]+b_o)$       \n",
    "- $c^{<t>} = \\Gamma u * \\tilde{c}^{<t>}+\\Gamma f*c^{<t-1>}$  \n",
    "- $a^{<t>} = \\Gamma o*c^{<t>}$ \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_7.PNG\" width=600 height=600> \n",
    "\n",
    "常用版本\n",
    "peehole connect（偷窥孔连接）——门值也可能受到$c^{<t-1>}$的影响   \n",
    "\n",
    "\n",
    "> ## Bidirectional RNN（BRNN）     \n",
    "```\n",
    "He said,\"Teddy bears are on sale\"\n",
    "在上述句子中，无法根据前三个单词判断Teddy是否是个人名\n",
    "```      \n",
    "无论是DNN,RNN,CNN都是单向传播的。   \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_8.PNG\" width=600 height=600>   \n",
    "如图所示，BRNN的前向传播过程是由1-2-3-4，$x^{<4>}$输入时计算反向的$a^{<4>}$同时输出$\\hat y^{<4>}$最后往回计算一直到$\\hat y^{<1>}$      \n",
    "\n",
    "> ## DRNN     \n",
    "\n",
    "符号:$a^{[l]}{<t>}$代表第$l$层的第$t$个时间步的激活值    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_1_9.PNG\" width=600 height=600> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK2           \n",
    "> ## 词汇表征(Word representation)                  \n",
    "\n",
    "One-hot编码的缺陷:模型的泛化能力会因为One-hot编码后各类词的内积都为0所以**认为同类词毫无关联**，比如apple与orange     \n",
    "\n",
    "**解决方法——词嵌入(word embeddings)**:利用特征化的方式表示各个词语     \n",
    "列索引为特征，行索引为词语，值表示词语在与这个特征的关联度     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_1.PNG\" width=600 height=600>\n",
    "\n",
    "> ## 使用词嵌入   \n",
    "\n",
    "1. 利用网络上或等等渠道获得的大量文本学习词嵌入(下载预训练过的词嵌入模型)    \n",
    "2. 将词嵌入模型迁移到具有少量标记数据的网络模型中  \n",
    "3. 可选择的：利用已有的数据微调已获得的词嵌入模型     \n",
    "\n",
    "> ## 词嵌入的特性     \n",
    "\n",
    "词嵌入能够实现词语间的类比推理     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_2.PNG\" width=600 height=600>       \n",
    "```\n",
    "需要推理Man-Woman&King-Queen\n",
    "```\n",
    "在词嵌入模型中，将Man与Woman的嵌入向量命名为$e_{Man}$与$e_{Woman}$，那么有向量运算:\n",
    "$$e_{Man}-e_{Woman}=e_{King}-e_{Queen}$$\n",
    "两对词语的词嵌入向量在Gender维度上距离相同。    \n",
    "\n",
    "类比推理可以理解为(实际应用中，该方法准确性不高):        \n",
    "$$Find\\ \\ word\\ \\ w:arg\\underset{{w}}{max}\\ \\ similarity(e_w,e_{Woman}-e_{Man}+e_{King})$$    \n",
    "\n",
    "一种相似度函数——余弦相似度:$sim(u,v)=\\cfrac{u^Tv}{||u||_2||v||_2}$      \n",
    "\n",
    "> ## 嵌入矩阵   \n",
    "\n",
    "当学习词嵌入模型时，实际上是在学习一个**嵌入矩阵**       \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_3.PNG\" width=600 height=600>      \n",
    "一个词嵌入矩阵$E$与一个词$j$的One-hot编码相乘实际上得到的是词嵌入矩阵中关于$j$的那一列$e_j$，在实际操作中，通常直接提取。\n",
    "\n",
    "> ## 学习词嵌入     \n",
    "\n",
    "通过建立一个语言模型的方法将嵌入矩阵与词One-hot编码相乘传入神经网络进行训练来训练词嵌入模型       \n",
    "网络会学到将类似词语的特征向量相似的时候结果会最优化\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_4.PNG\" width=600 height=600>     \n",
    "\n",
    "不同的选择上下文的方法:     \n",
    "1. 前四个单词   \n",
    "2. 前四个与后四个单词  \n",
    "3. 上一个单词   \n",
    "4. 邻近的一个单词(skip grams)    \n",
    "\n",
    "当学习语言模型时——方法1，学习词嵌入时——方法1、2、3\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_4.PNG\" width=600 height=600>     \n",
    "\n",
    "> ## Word2Vec    \n",
    "\n",
    "Word2Vec模型中，主要有Skip-Gram和CBOW两种模型，从直观上理解，Skip-Grams是**给定input word来预测上下文**。而CBOW是给定上下文，来预测input word。   \n",
    "```\n",
    "例:i want a glass of orange juice to go along with my cereal\n",
    "context   target\n",
    "\n",
    "orange    juice\n",
    "orange    glass\n",
    "orange    my\n",
    "```\n",
    "这是一个监督学习模型，但不是为了学习这个模型，而是利用这个监督学习来学习词嵌入模型    \n",
    "\n",
    "### Word2Vec目标\n",
    "Word2Vec模型实际上分为了两个部分。第一部分为建立模型，第二部分是通过模型获取嵌入词向量。      \n",
    "Word2Vec的整个建模过程实际上与自编码器（auto-encoder）的思想很相似，即先基于训练数据构建一个神经网络，当这个模型训练好以后，并不会用这个训练好的模型处理新的任务，真正需要的是这个模型通过训练数据所学得的参数，例如**隐层的权重矩阵——这些权重在Word2Vec中实际上就是我们试图去学习的“word vectors”**。    \n",
    "### Word2Vec过程     \n",
    "```\n",
    "例:i want a glass of orange juice to go along with my cereal\n",
    "```\n",
    "\n",
    "1. 首先在一个句子中选择一个input word，定义一个**窗口大小**(左右侧几个单词作为选择输出单词的界限)与一个**选择个数**(在窗口中选择的单词个数).组成**单词对(input_word,ouput_word)**   \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_6.PNG\" width=600 height=600>     \n",
    "\n",
    "2. 对每个**输入单词与输出单词进行One-hot编码**，输入单词输入隐层神经元(没有激活函数，线性相乘)——参数即**嵌入矩阵**，最后输入softmax得出结果，利用输出单词进行参数训练。   \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_7.PNG\" width=600 height=600>\n",
    "\n",
    "3. 具体如下:    \n",
    "context:$c$     \n",
    "target:$t$    \n",
    "将输入单词context$c$进行one-hot编码得到$O_c$     \n",
    "$O_c$乘以嵌入矩阵$E$,得到$e_c=EO_c$    \n",
    "$e_c$输入softmax层得到目标单词的预测向量$\\hat y$      \n",
    "\n",
    "其中:Softmax结果——$p(t|c)=\\cfrac{e^{\\theta_t^Te_c}}{\\sum_j^v\\theta_j^Te_c}$,$v$为词汇表长度，$\\theta_t$为单词$t$有关一个参数         \n",
    "该方法主要的难点在于，语料库过长，最后在求softmax层时需要很大的算例，所以通常使用分级softmax。\n",
    "### 直觉上的理解\n",
    "\n",
    "如果两个不同的单词有着非常相似的“上下文”（也就是**窗口单词**很相似，比如“Kitty climbed the tree”和“Cat climbed the tree”），那么通过模型训练，这两个单词的**嵌入向量将非常相似**。       \n",
    "\n",
    "这种方法实际上也可以帮助你进行词干化(stemming)(词干化(stemming)就是去除词缀得到词根的过程。)，例如，神经网络对”ant“和”ants”两个单词会习得相似的词向量。\n",
    "\n",
    "> ## 负采样     \n",
    "\n",
    "```\n",
    "例:i want a glass of orange juice to go along with my cereal\n",
    "```\n",
    "\n",
    "### 负采样的训练流程\n",
    "\n",
    "1. 首先选取一个词语与其上下文，该例子中选择```orange-juice```，作为一个**正样本**，目标值为**1**。然后从词汇表中随机选择$k$(小数据集：k=5-20；大数据集:k=2-5)个词语与在样本中选择的词语```orange```组成**负样本**，目标值为**0**(随机选择的词语在上下文中也标记为负样本)     \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_8.PNG\" width=200 height=200>    \n",
    "\n",
    "```\n",
    "x(c,t)             y\n",
    "\n",
    "(orange,juice)     1\n",
    "(orange,king)      0\n",
    "(orange,book)      0\n",
    "(orange,the)       0\n",
    "(orange,of)        0\n",
    "\n",
    "```\n",
    "\n",
    "2. 利用Logistics回归训练，$P(1|c,t)=\\sigma(\\theta_t^Te_c)$,神经网络图如下:    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_9.PNG\" width=500 height=500>     \n",
    "先将词语$c$进行one-hot编码，然后乘以嵌入矩阵$E$得到$e_c$，最后输入到宽度为**单词列表长度**的二分类层当中，每个逻辑回归(神经元)输出一个结果。这样每批样本只需要训练$k+1$个词语对应的神经元的二元分类问题(**k个负样本和1个正样本**)\n",
    "\n",
    "### 如何进行负采样     \n",
    "1. 通过词频(单词的经验频率)$P(w_i)=f(w_i)$进行采样(某些词的词频特别高,比如一些代词，介词等等)   \n",
    "2. 通过词汇表的均匀分布进行随机采样(缺乏代表性)    \n",
    "\n",
    "**解决方法**:$P(w_i) = \\cfrac{f(w_i)^{\\frac{3}{4}}}{\\sum_j^vf(w_j)^{\\frac{3}{4}}}$     \n",
    "\n",
    "> ## Glove算法(Glove:Global vector for word representation)     \n",
    "\n",
    "$c,t$是上下文中的词语与目标词汇    \n",
    "设$x_{ij}$是词语$i$在词语$j$的上下文出现的次数($i-t,j-c$)    \n",
    "$x_{ij}=x_{ji}$在某些上下文定义中不成立，例如$c$是$t$的上一个单词    \n",
    "\n",
    "在Glove算法中，上下文的定义是一个左右长度相等的窗口，因此$x_{ij}=x_{ji}$\n",
    "\n",
    "### Glove算法模型:    \n",
    "$$minimize\\ \\ \\sum_i^v\\sum_j^vf(x_{ij})(\\theta_i^Te_j+b_i+b_j-Log(x_{ij}))$$       \n",
    "\n",
    "**权重项$f(x_{ij})$的作用**:         \n",
    "\n",
    "1. $f(x_{ij})=0,if\\ \\ x_{ij}=0(log(x_{ij})\\rightarrow \\infty)$，这里认为$0log0=0$    \n",
    "2. $f(x_{ij})$函数的选择可以令词频较高的词语不会获得那么大的权重(the,of,a一类的代词以及介词)，也让一些词频低的词不会获得那么小的权重(durion等)     \n",
    "\n",
    "### 注意事项     \n",
    "嵌入矩阵所代表的轴可能无法理解，甚至相互之间不正交     \n",
    "\n",
    ">## 情绪分类     \n",
    "\n",
    "例子:    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_10.PNG\" width=500 height=500>      \n",
    "给定一个句子为输入值，输出值为评价星级。     \n",
    "\n",
    "### DNN方法:     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_11.PNG\" width=500 height=500>        \n",
    "将每个词语进行One-hot编码后于嵌入矩阵相乘得到词嵌入向量，然后输入一个平均值运算，最后输入一个星级的softmax分类器。     \n",
    "\n",
    "平均值运算会将所有词的评价程度平均后得出每个星级评价的概率值。但存在一个缺陷——没有考虑语序，当遇到左下角的句子出现了很多个good时会存在偏差。     \n",
    "\n",
    "### RNN方法     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_12.PNG\" width=500 height=500>      \n",
    "\n",
    "> ## 词嵌入除偏(性别、种族的偏见而不是bias)    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_13.PNG\" width=500 height=500>      \n",
    "训练过的词嵌入模型可能会显示出性别、种族等的歧视(如上图)     \n",
    "\n",
    "### 减少偏见的程序(以性别为例):   \n",
    "**1. 鉴定我们需要减少的偏见**     \n",
    "$e_{man}-e_{female}$          \n",
    "$e_{boy}-e_{girl}$    \n",
    "$\\vdots$    \n",
    "$求出以上差的平均值$\n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_2_14.PNG\" width=200 height=200>    \n",
    "横轴是偏见趋势，纵轴是无偏见趋势(实际情况一般大于一维)      \n",
    "\n",
    "**2. 中和步骤:对于每一个非定义性的单词，消除偏见**     \n",
    "比如babysister,computer programmer等没有明确的性别含义，向非偏见轴零点移动\n",
    "\n",
    "**3. 均衡步骤**        \n",
    "让grandmother-grandfather等词语于非偏见轴保持等距离\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK3\n",
    "## Seq2Seq\n",
    "机器翻译任务——将一串法语翻译成英语     \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_1.PNG\" width=500 height=500>     \n",
    "\n",
    "**定义两个网络:Encoder与Decoder**    \n",
    "**Encoder**:接受输入序列，输出一个向量表征这个序列(编码)     \n",
    "**Decoer**:将Encoder的输出做为输入，每步输出一个值——直到输出序列的结尾。    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_2.PNG\" width=500 height=500>       \n",
    "\n",
    "语言模型与机器翻译的差别:     \n",
    "- 语言模型使用是个随机输入与**随机采样**来形成一个文本序列。\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_3.PNG\" width=500 height=500>\n",
    "- 机器翻译的输出部分接受的**不是随机输入**，而是一个**编码向量**。并且与语言模型不同的是，机器翻译追求$P(y^{<1>},y^{<2>}...y^{<T_y>}|x)$的最优值，因此在每次输出后不是采用**随机采样**，而是**集束搜索**       \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_4.PNG\" width=500 height=500>              \n",
    "\n",
    "## Beam Search（集束搜索）                \n",
    "\n",
    "1. 第一步:集束搜索在Decoder每次输出时会考虑可能性最大的$n$个单词，$n$是集束宽度($Beam \\ width$)\n",
    "2. 第二步:由于第一步搜索出了$n$个词语，得到第一个输出的概率$P(y^{<1>}|x)$;根据第一步$n$个输出，继续估计下一个词语，得到概率值$P(y^{<1>},y^{<2>}|x)$，这样一共会有$Beam\\ width*Number\\ of\\ words$个词语，再根据条件概率选出其中最大的$n$个词语。        \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_5.JPG\" width=500 height=500>\n",
    "\n",
    "### 改进集束搜索的技巧       \n",
    "\n",
    "1. 集束搜索中，每个词语的概率值都是小于1，因此连乘可能导致数值下溢——解决方法:取对数   \n",
    "\n",
    "$$P(y^{<1>},y^{<2>}...y^{<T_y>}|x) = P(y^{<1>}|x)P(y^{<2>}|x,y^{<1>})....P(y^{<T_y>}|x,y^{<1>},...y^{<T_y-1>})$$            \n",
    "\n",
    "$$argmaxP(y^{<1>},y^{<2>}...y^{<T_y>}|x)$$    \n",
    "\n",
    "$$\\Rightarrow \\underset{y}{arg\\ max}\\prod P(y^{<t>}|x,y^{<1>}...y^{<t-1>})$$    \n",
    "\n",
    "$$\\Rightarrow \\underset{y}{arg\\ max}\\sum log(P(y^{<t>}|x,y^{<1>}...y^{<t-1>}))$$       \n",
    "\n",
    "2. 集束搜索中，最优化函数倾向于更短的句子——解决方法:归一化(除以句子长度或句子长度的$\\alpha(=0.7,usually)$ 次幂)      \n",
    "\n",
    "$$\\frac{1}{T_y^\\alpha} P(y^{<1>},y^{<2>}...y^{<T_y>}|x)$$    \n",
    "\n",
    "\n",
    "### 集束搜索的误差分析\n",
    "\n",
    "机器翻译句子:@@jane is a pussy boy@@@@@@—— $\\hat y$              \n",
    "人类翻译句子:##jane is a bad boy######—— $y^*$ \n",
    "\n",
    "利用RNN计算两个句子的概率$P(y\\hat|x)$与$P(y^* |x)$     \n",
    "那么存在两种情况    \n",
    "1.$P(y\\hat|x)>P(y^* |x)$     \n",
    "网络结构出问题    \n",
    "2. $P(y\\hat|x)<P(y^* |x)$    \n",
    "$Beam\\ Search$出问题\n",
    "\n",
    "## BLEU   \n",
    "```\n",
    "翻译一个法语句子，有两个参考的人工翻译  \n",
    "\n",
    "Reference 1: The cat is on the mat.\n",
    "Reference 2:There is a cat on the mat.\n",
    "```\n",
    "\n",
    "Bleu的原理:机器翻译的结果接近人工的参考翻译结果——看生成的词是否出现在参考翻译中。    \n",
    "\n",
    "```\n",
    "percision:机器翻译中的单词出现在参考翻译中的个数/总单词个数   \n",
    "modified:机器翻译中的单词出现在参考翻译中的个数/总单词个数，\n",
    "      每个单词有个计分上限，比如\"the\"，在翻译1中出现2次，翻译2中出现1次，计分上限为2\n",
    "```\n",
    "\n",
    "\n",
    "### n-grams的Bleu   \n",
    "```\n",
    "Example: \n",
    "Reference 1: The cat is on the mat.\n",
    "Reference 2: There is a cat on the mat.\n",
    "MT output: The cat the cat on the mat.\n",
    "         count   count_clip\n",
    "the cat    2       1\n",
    "cat the    1       0\n",
    "cat on     1       1\n",
    "on the     1       1\n",
    "the mat    1       1\n",
    "\n",
    "BLEU = 4/6\n",
    "```\n",
    "\n",
    "对于**n-grams**而言:   \n",
    "\n",
    "$$P_n = \\cfrac{\\sum_{n-grams\\in \\hat y }Content_{clip} (n-grams)}{\\sum_{n-grams\\in \\hat y}Content(n-grams)}$$\n",
    "\n",
    "### Combined Bleu Scores      \n",
    "\n",
    "$$(BP)exp(\\frac{1}{4}\\sum_{n=1}^4P_n),BP-brief\\ penalty(简短惩罚)$$    \n",
    "\n",
    "$$\n",
    "BP =\\begin{cases}  \n",
    "1&,if MT_{output\\ length} > reference_{output\\ length}\\\\\n",
    "exp(1 — MT_{output\\ length}/reference_{output\\ length})&,otherwise\n",
    "\\end{cases} \n",
    "$$    \n",
    "\n",
    "\n",
    "## 注意力模型     \n",
    "    \n",
    "\n",
    "当人在翻译句子的时候，不会一次性翻译一页，而是一段一段翻译。    \n",
    "在Encoder-Decoder模型当中，长段落会导致编码在压缩向量时产生损失，相比于短段落BLEU分值低。    \n",
    "\n",
    "考虑下面这样的一个模型    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_6.png\" width=500 height=500>    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_7.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "## 语音辨识    \n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_8.png\" width=500 height=500>    \n",
    "\n",
    "## Trigger word     \n",
    "不平衡数据集的解决方法——听到trigger word之后持续输出一段时间的1    \n",
    "\n",
    "<img style=\"float: center;\" src=\"course_5_pics/WEEK_3_9.png\" width=500 height=500>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
